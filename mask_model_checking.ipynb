{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563f523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import timm\n",
    "\n",
    "assert timm.__version__ == \"0.3.2\" # 버전 체크 - timm 0.3.2 버전 확인\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "\n",
    "# 커스텀 유틸리티 모듈들 임포트\n",
    "import util.lr_decay as lrd  # 학습률 스케줄링\n",
    "import util.misc as misc  # 기타 유틸리티 함수들\n",
    "from util.datasets import build_dataset  # 데이터셋 생성\n",
    "from util.pos_embed import interpolate_pos_embed  # 위치 임베딩 보간\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "\n",
    "import models_vit  # Vision Transformer 모델\n",
    "\n",
    "from engine_finetune import train_one_epoch, evaluate  # 훈련 및 평가 엔진\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "801f0434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from torchvision import datasets, transforms\n",
    "import PIL\n",
    "def build_transform(is_train):\n",
    "    input_size = 224\n",
    "    mean = IMAGENET_DEFAULT_MEAN\n",
    "    std = IMAGENET_DEFAULT_STD\n",
    "    # train transform\n",
    "\n",
    "    # eval transform\n",
    "    t = []\n",
    "    if input_size <= 224:\n",
    "        crop_pct = 224 / 256\n",
    "    else:\n",
    "        crop_pct = 1.0\n",
    "    size = int(input_size / crop_pct)\n",
    "    t.append(\n",
    "        transforms.Resize(size, interpolation=PIL.Image.BICUBIC),  # to maintain same ratio w.r.t. 224 images\n",
    "    )\n",
    "    t.append(transforms.CenterCrop(input_size))\n",
    "\n",
    "    t.append(transforms.ToTensor())\n",
    "    t.append(transforms.Normalize(mean, std))\n",
    "    return transforms.Compose(t)\n",
    "\n",
    "\n",
    "def build_dataset(is_train):\n",
    "    transform = build_transform(is_train)\n",
    "    data_path = \"/data/docparser_mh/temp/data/ImageNet\"\n",
    "    root = os.path.join(data_path, 'train' if is_train else 'val')\n",
    "    dataset = datasets.ImageFolder(root, transform=transform)\n",
    "\n",
    "    print(dataset)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96cfa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 50000\n",
      "    Root location: /data/docparser_mh/temp/data/ImageNet/val\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)\n",
      "               CenterCrop(size=(224, 224))\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      "           )\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     22\u001b[0m data_loader_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     23\u001b[0m     dataset_val, sampler\u001b[38;5;241m=\u001b[39msampler_val,\n\u001b[1;32m     24\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# 검증 시에는 모든 샘플 사용\u001b[39;00m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Vision Transformer 모델 생성\u001b[39;00m\n\u001b[1;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m models_vit\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit_base_patch16\u001b[39m\u001b[38;5;124m\"\u001b[39m](\n\u001b[0;32m---> 32\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mnb_classes,\n\u001b[1;32m     33\u001b[0m     drop_path_rate\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdrop_path,\n\u001b[1;32m     34\u001b[0m     global_pool\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mglobal_pool,\n\u001b[1;32m     35\u001b[0m     mask_ratio\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmask_ratio,\n\u001b[1;32m     36\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "FineTune = \"/data/docparser_mh/temp/mae_temp/output_dir/checkpoint-1.pth\"\n",
    "# 재현성을 위한 시드 고정\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "cudnn.benchmark = True  # CuDNN 성능 최적화\n",
    "\n",
    "# 훈련 및 검증 데이터셋 구성\n",
    "# dataset_train = build_dataset(is_train=True, args=args)\n",
    "dataset_val = build_dataset(is_train=False)\n",
    "\n",
    "# 분산 훈련을 위한 데이터 샘플러 설정\n",
    "\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "# 텐서보드 로거 설정\n",
    "log_writer = None\n",
    "\n",
    "# 데이터 로더 생성\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, sampler=sampler_val,\n",
    "    batch_size=4,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=False  # 검증 시에는 모든 샘플 사용\n",
    ")\n",
    "\n",
    "# Vision Transformer 모델 생성\n",
    "model = models_vit.__dict__[\"vit_base_patch16\"](\n",
    "    num_classes=1000,\n",
    "    drop_path_rate=0.1,\n",
    "    global_pool=True,\n",
    "    mask_ratio=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4421a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(FineTune, map_location='cpu')\n",
    "checkpoint_model = checkpoint['model']\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
